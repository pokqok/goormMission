위키피디아 데이터(KorQuAD) 기반 LLM 질의응답 서비스 (with Pinecone & KoBERT)
1. 프로젝트 설명
본 프로젝트는 Hugging Face의 squad_kor_v1 (KorQuAD 1.0) 데이터셋을 활용하여 사용자의 질문에 대한 답변을 생성하는 RAG(Retrieval-Augmented Generation) 기반의 LLM 서비스입니다. FastAPI를 사용하여 REST API 형태로 서비스를 제공하며, Pinecone 벡터 데이터베이스와 KoBERT 임베딩 모델을 활용하여 사용자의 질문과 가장 관련성이 높은 위키피디아 문서 일부를 검색하고, 이를 바탕으로 Hugging Face의 오픈소스 LLM이 정확하고 근거 있는 답변을 생성합니다.

주요 기능
데이터 기반 답변: KorQuAD 데이터셋에 포함된 위키피디아 문서 내용을 기반으로 답변합니다.

REST API 제공: /ask 엔드포인트를 통해 JSON 형식으로 질문을 받고 답변을 반환합니다.

출처 제공: 답변 생성 시 참고한 위키피디아 문서의 원본 내용을 함께 제공합니다.

질의 범위 제한: Pinecone 벡터 검색의 코사인 유사도 점수를 활용하여 질문과 관련 없는 문서가 검색될 경우, 답변 생성을 제한하고 사용자에게 안내합니다.

2. 구현 아키텍처 및 접근 방식
이 시스템은 검색(Retrieval)과 생성(Generation) 두 단계로 구성된 RAG 아키텍처를 따르며, 벡터 검색 엔진으로 Pinecone을 사용합니다.

1. 데이터 처리 (오프라인)

data_processing.py:

datasets 라이브러리를 사용하여 squad_kor_v1 데이터셋을 로드합니다.

중복을 제거한 위키피디아 context들을 RecursiveCharacterTextSplitter를 사용해 의미 있는 단위(chunk)로 분할합니다.

skt/kobert-base-v1 임베딩 모델을 사용하여 각 텍스트 청크를 벡터로 변환합니다.

변환된 벡터들을 Pinecone 클라우드 데이터베이스의 지정된 인덱스에 업로드하고 저장합니다. 이 과정은 최초 1회 또는 데이터 업데이트 시 실행합니다.

2. API 서버 구동 및 질의응답 (온라인)

app.py:

FastAPI 서버가 시작될 때, KoBERT 임베딩 모델과 답변 생성을 위한 LLM(EleutherAI/polyglot-ko-1.3b)을 메모리에 로드합니다.

LangChain의 Pinecone 통합 라이브러리를 통해 Pinecone 인덱스에 연결합니다.

사용자가 /ask 엔드포인트로 질문을 보내면, 다음 RAG 파이프라인이 실행됩니다.

Retrieve: 질문을 임베딩하여 벡터로 만든 후, Pinecone 인덱스에서 코사인 유사도 기반으로 가장 유사한 문서 청크를 검색합니다.

Augment: 검색된 문서 청크와 사용자 질문을 프롬프트 템플릿에 결합하여 LLM에 전달할 입력값을 만듭니다.

Generate: 최종 프롬프트를 LLM에 전달하여 답변을 생성하고, 사용자에게 JSON 형식으로 반환합니다.

기술 스택
언어: Python 3.9+

API 서버: FastAPI, Uvicorn

LLM/Embedding: Hugging Face Transformers, Datasets, LangChain

벡터 DB: Pinecone

핵심 모델:

Embedding Model: skt/kobert-base-v1 (768 차원)

Generative LLM: EleutherAI/polyglot-ko-1.3b

3. 설치 및 실행 방법
사전 요구사항
Python 3.9 이상

Git

Pinecone 계정 및 API 키 (Pinecone 홈페이지에서 가입 후 무료 API 키를 발급받으세요.)

설치 과정
GitHub 저장소 복제

git clone <your-repository-url>
cd <your-repository-name>

가상 환경 생성 및 활성화

python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

필요 라이브러리 설치
requirements.txt 파일을 아래 내용으로 생성하고, pip로 설치합니다.

fastapi
uvicorn[standard]
langchain
langchain-community
langchain-huggingface
langchain-pinecone
datasets
pinecone-client
sentence-transformers
torch
transformers
accelerate
bitsandbytes

pip install -r requirements.txt

환경 변수 설정
프로젝트를 실행하기 전에 Pinecone API 키와 환경(Environment) 정보를 환경 변수로 설정해야 합니다. 터미널에서 다음 명령어를 실행하거나, .env 파일을 사용하세요.

export PINECONE_API_KEY="YOUR_PINECONE_API_KEY"
export PINECONE_ENVIRONMENT="YOUR_PINECONE_ENVIRONMENT" # 예: "gcp-starter"

실행 방법
데이터 전처리 및 Pinecone 인덱스 채우기

중요: 임베딩 모델이 변경되었으므로, 기존 Pinecone 인덱스를 삭제하고 다시 생성하는 것을 권장합니다.

프로젝트 루트 디렉토리에서 아래 명령어를 실행하여 Pinecone에 데이터를 새로 업로드합니다.

python data_processing.py

API 서버 실행

데이터 처리가 완료된 후, 아래 명령어로 FastAPI 서버를 시작합니다.

uvicorn app:app --host 0.0.0.0 --port 8000

서버가 성공적으로 실행되면, 브라우저에서 http://127.0.0.1:8000/docs 로 접속하여 API 문서를 확인할 수 있습니다.

4. API 사용 예시
curl을 사용한 API 요청 예시는 이전과 동일합니다.

요청 (Request):

curl -X 'POST' \
  '[http://127.0.0.1:8000/ask](http://127.0.0.1:8000/ask)' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "question": "대한민국의 수도는 어디인가요?"
}'

응답 (Response):

{
  "retrieved_document": "대한민국(大韓民國)은 동아시아의 한반도 남부에 자리한 민주공화국이다. 대한민국의 국기는 태극기이며, 국가는 애국가이고, 수도는 서울특별시이다. 공용어는 한국어와 한국 수어이다. 2015년 인구주택총조사에 따르면 총 인구는 51,069,375명이며, 이는 세계 27위이다. 대한민국은 1948년 8월 15일에 정부수립을 선포하며 독립하였다.",
  "question": "대한민국의 수도는 어디인가요?",
  "answer": "대한민국의 수도는 서울특별시입니다.",
  "score": 0.9123
}

score는 검색된 문서와 질문 간의 코사인 유사도를 나타내며, 1에 가까울수록 관련성이 높습니다.